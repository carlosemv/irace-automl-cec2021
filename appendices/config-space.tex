\section{\isklearn configuration space}
\label{app:config-space}

Algorithmic options for feature extraction in \isklearn's template (given in Table~\ref{tb:components}) are dimensionality reduction algorithms, and depend on the density of the given dataset: for sparse datasets, \textit{truncated singular value decomposition}~(SVD) is the single option; otherwise, either \textit{principal component analysis}~(PCA), \textit{independent component analysis}~(ICA), or \textit{dictionary learning}~(DL) might be used.

As for \texttt{\small Selection}, we have the \textit{univariate} approach, which retrieves a certain percentile of features based on a scoring function: either the ANOVA F-values~\cite{anova} or the Mutual Information~\cite{mutual-info} between features and target. This proportion of features retrieved is a hyperparameter itself, and can have any integer value between 1 and 99 (inclusive). Multivariate selection can also be performed recursively, using the \textit{recursive feature elimination}~(RFE) approach. As for the \textit{multivariate} approach, a feature importance model is fit using any of the following predictors: \textit{decision trees}~(DT,~\cite{decisiontrees}), \textit{random forests}~(RF,~\cite{randomforests}), and \textit{support vector machines}~(SVM), for either classification or regression datasets, and; \textit{linear regression}~(LR), for regression datasets only. These were choosen due to their balance between efficacy and efficiency when used with their suggested default parameters, as we do when using these predictors in the context of feature selection, since the configuration of nested models is a complex aspect to be addressed in future work.

For \texttt{\small Prediction}, the second main component in this template, a subset of all estimators in \isklearn are considered. This subset includes the ones already mentioned for multivariate selection~(DT, RF, SVM, and LR), as well as \textit{k-nearest neighbors}~(kNN), \textit{multilayer perceptron}~(MLP,~\cite{mlp}), \textit{AdaBoost}~(AB,~\cite{adaboost}), and \textit{logistic regression}~(LR\footnote{LR stands for Linear Regression in the case of regression tasks, and Logistic Regression in the case of classification tasks}). These represent most families of ML prediction algorithms commonly used, namely generalized linear models, trees, manifold learning, neural networks, and ensembles. Most of these are available for either classification or regression tasks when possible, as distinguished in Table~\ref{tb:components}.

\input{appendices/configuration-table.tex}

Nearly all algorithms present in the template described expose associated hyperparameters that need to be configured by \irace. Those hyperparameters, as well as their selected domains, are detailed in Table~\ref{tb:parameters}, grouped by algorithm. Further information on the hyperparameters of each estimator considered can be found below. For further specifics on these hyperparameters, we refer to the original papers for these algorithms, as well as to the documentation for \sklearn.

\begin{description}
\item[KNN:] The number $k$ of neighbors is provided as a hyperparameter, as well as whether to use uniform or distance-based weights for each neighbor.

\item[AdaBoost~(AB):]
For both regression and classification, \irace must configure the number of base estimators and the learning rate. For regression tasks, three different loss functions are considered, used to update the weights after each iteration.

\item[Decision trees~(DT):]
\irace needs to configure (i)~the proportion of the features that can be used to build the tree, and (ii)~the minimum number of samples required for a leaf node (given as a fraction of the total number of samples). Optionally, \irace may configure a maximum depth value for the tree. The criterion used to measure the quality of a split is also provided as a hyperparameters, with different possible values for classification and regression tasks -- gini or entropy for classification, and mean squared error (MSE) or mean absolute error~(MAE) for regression.

\item[Random forests~(RF):]
The configuration space for random forests is a superset of the space for decision trees. Besides all of the hyperparameters from DT, \irace must also configure the number of estimators (trees) to be used.

\item[Support vector machines~(SVM):]
\irace must configure the penalty parameter of the error term~($C$), the kernel to be used, and its associated hyperparameters $\gamma$ (in the case of non-linear kernels). Also, for a polynomial kernel, the degree of the polynomial function is configured.

\item[Multilayer perceptron~(MLP):]
\irace must configure the number of hidden layers and the number of neurons in each layer. The activation function, solver (or optimizer), and L2 penalty must also be configured. For solvers SGD and Adam, the initial learning rate is also configured and, specifically for SGD, a learning rate schedule is chosen.

\item[logistic regression~(LR):]
For this algorithm, the parameters tuned are: the inverse of regularization strength~($C$); the optimizer (or solver) used in the optimization problem; whether to fit a binary problem for each label (\textit{ovr}), otherwise the loss minimised is the multinomial loss fit across the entire probability distribution (\textit{multinomial}); maximum number of iterations for the optimizers; the norm used in the penalization; and whether to use dual or prime formulation.
\end{description}

Note that in implementing this configuration space the logarithmic transformation was adopted, being applied to real-valued hyperparameters that present a very large range of values, following relevant findings in this topic~\cite{franzin2017effect}. As an example, the L2 penalty hyperparameter in MLPs is then modeled as a surrogate parameter $\alpha \in [-5,4]$, which is then mapped by \isklearn during candidate evaluation back to \mbox{L2 $\in [10^{-5}, 10^4]$}.

\section{Sampling configuration setup}
\label{app:sampling-conf}

Choosing values for $k$ and $p$ (see discussion on problem sampling in Section~\ref{sec:config-setup}) is a manual design stage that needs to account for the characteristics of the particular dataset one wants to address. The choice of $k$ should avoid meta-folds with too little samples, particularly if there is some level of class unbalance or a low sample-per-class ratio. Regarding $p$, a large value would greatly increase the computational cost for evaluating a single candidate once. Ideally, $p$ should be set to a small value. Yet, if $k$ is small, having a value of $p$ too close to the value of $k$ would mitigate the benefits of the proposed approach, since nearly the same meta-folds would be evaluated every time. 