%!TEX root = ../thesis.tex
{\renewcommand{\baselinestretch}{1.3}
\begin{table}[!t]
\centering
\caption{Configuration space of the hyperparameters associated with the algorithms comprising \isklearn.}
\label{tb:parameters}
    \scalebox{0.8}{\begin{tabular}{p{2.5cm}lp{4.9cm}}
    \hline
    \textbf{Algorithm } & \textbf{Parameter} & \textbf{Space}\\ \hline

    \multirow{2}{*}{KNN} & $k$ & $\{1, \dotsc, 100\}$\\
    \cline{2-3}
    & \textit{weights} & $\{\text{uniform}, \text{weighted}\}$\\
    \hline

    \multirow{2}{*}{AB} & $\Nest$ & $\{2, \dotsc, 500\}$\\
    \cline{2-3}
    & \textit{learning rate} & $[0.01, 1.0]$\\
    \cline{2-3}
    & \textit{loss function} & $\{\text{linear}, \text{square}, \text{exponential}\}$\\
    \hline

    \multirow{5}{*}{DT \& RF} & \textit{max features}& $[0.01, 1.0]$\\
    \cline{2-3}
    & \textit{min samples leaf} & $[0.01, 0.5]$\\
    \cline{2-3}
    & \textit{max depth} & \emph{none} or $\{2, \dotsc, 50\}$\\
    \cline{2-3}
    & \textit{classification criterion} & $\{\text{gini}, \text{entropy}\}$\\
    \cline{2-3}
    & \textit{regression criterion} & $\{\text{MSE}, \text{MAE}\}$\\
    \hline

    RF & \Nest & $\{2, \dotsc, 1000\}$\\
    \hline

    \multirow{4}{*}{SVM}& C & $[1e\mathrm{-4}, 1e\mathrm{5}]$ \\
    \cline{2-3}
    & \textit{kernel} & \{\textit{linear}, \textit{polynomial}, \textit{RBF}, \textit{sigmoid}\} \\
    \cline{2-3}
    & $\gamma$ & $[1e\mathrm{-5}, 10]$ \\
    \cline{2-3}
    & \textit{polynomial degree} & $\{1, \dotsc, 10\}$ \\
    \hline

    \multirow{7}{1cm}{MLP} & \textit{hidden layers} & \{1, 2, 3\}\\
    \cline{2-3}
    & $\textit{nodes}_1$, $\textit{nodes}_2$, $\textit{nodes}_3$ & $\{3, \dotsc, 500\}$\\
    \cline{2-3}
    & \textit{activation function} & \{\textit{identity, logistic, tanh, ReLU}\}\\
    \cline{2-3}
    & \textit{optimizer} & \{\textit{lbfgs, sgd, adam}\}\\
    \cline{2-3}
    & \textit{L2 penalty} & $[1e\mathrm{-5}, 1e\mathrm{4}]$\\
    \cline{2-3}
    & \textit{initial learning rate} & $[1e\mathrm{-6}, 1]$\\
    \cline{2-3}
    & \textit{learning rate} & \{\textit{constant, invscaling, adaptive}\}\\
     \hline

    \multirow{6}{*}{Logistic Regression}& C & $[1e\mathrm{-4}, 1e\mathrm{5}]$ \\
    \cline{2-3}
    & \textit{optimizer} & \{\textit{newton-cg, lbfgs, liblinear, sag, saga}\} \\
    \cline{2-3}
    & \textit{multi-class} & \{\textit{ovr, multinomial}\} \\
    \cline{2-3}
    & \textit{maximum iterations} & $\{100, \dotsc, 1000\}$ \\
    \cline{2-3}
    & \textit{penalty} & \{\textit{l1, l2}\} \\
    \cline{2-3}
    & \textit{dual formulation} & \{\textit{true, false}\} \\
    \cline{2-3}
    \hline
    \end{tabular}}
\end{table}
}
