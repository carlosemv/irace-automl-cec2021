%!TEX root = ../main.tex
\section{\isklearn: a fully-functional AutoML system}
\label{sec:isklearn}

As previously discussed, an AutoML approach based on algorithm configuration comprises (i)~a configuration space, including a meta-description of a portfolio and valid domains for the hyperparameters of the algorithms that comprise it, and; (ii)~an experimental setup to evaluate candidates, enabling the AutoML approach to select/configure an algorithm that is high-performing for the input dataset. In this section, we propose a configuration space and setup, which we will assess on a set of relevant ML datasets in Section~\ref{sec:results}.

\input{sections/space}
%\input{sections/configuration}
%\input{sections/setup}

\subsection{Configuration setup}
\label{sec:config-setup}

As previously discussed, the most important factors comprising a configuration setup concern the~(i)~problem samples provided; (ii)~performance metric adopted, and; (iii)~resource limits allowed. Below, we discuss each of these topics, starting from problem sampling, where our contributions lie;

\textbf{Problem samples.} AutoML through algorithm configuration requires three sampling levels. The top level evaluates the generalization of the AutoML approach; following the literature, we adopt holdout for this stage~\cite{autoweka,auto-sklearn}. We refer to this split as \emph{seen} and \emph{test}, since the configurator is never provided test samples. Conversely, the bottom level sampling evaluates the generalization of a candidate configuration on a subset of problem samples. In the literature, Auto-WEKA and \autosklearn once again adopt holdout. Conversely, we generalize this sampling level and test 5-fold cross-validation instead. Though this change could increase the computational cost to train a single candidate configuration, it improves the application of the AutoML systems to time series (TS) problems. More precisely, walk-forward cross-validation is a better fit for training time series models than holdout.

Finally, the mid-level sampling provides variability to the configuration process, defining what samples are seen by the bottom level sampling when a candidate configuration must be evaluated. An extreme alternative is to provide each candidate evaluation the whole dataset, at a significant computational cost. In addition, the variability between runs from a single configuration on the whole dataset is expected to be reduced for some predictors, even if the dataset is shuffled every time. The other extreme alternative would be to provide candidate configurations as little samples as possible. %~($k$, in the case of $k$-fold cross validation). 
Though fast, \irace would hardly see enough samples to avoid overfitting. %selecting candidates that overfit.

In the literature, Auto-WEKA and \autosklearn partition \emph{seen} samples into $k$ folds for this mid-level sampling. For clarity, we will dub these folds \emph{meta-folds}. Concretely, every time the configurator must evaluate a candidate, it is provided a single meta-fold, which is then subject to the bottom level sampling. Here, we also split the dataset into $k$ meta-folds, but again we generalize the sampling and provide the configurator $p$ meta-folds at a time. The evaluation is performed using the bottom sampling on each of the $p$ meta-folds independently, and results are averaged by the configurator. We remark that both our mid-level and bottom level sampling methods may be more expensive than those used by Auto-WEKA and \autosklearn. Our goal is to provide more samples to the training phase of each individual model, particularly when the dataset considered is not significantly large or there exists some level of class imbalance. Further discussion on how to set $k$ and $p$ is provided as supplementary material.

\textbf{Performance metric.} The choice of performance metric is more related to the problem one wants to address than to the nature of the configurator considered. For regression, typical choices include $R^2$ or (R)MSE. For classification, configuration for balanced datasets may adopt the traditional accuracy metric, whereas configuration for unbalanced datasets may benefit more from Matthews correlation coefficient~(MCC).

\textbf{Resource limits.} \isklearn does not present a priori concerns with memory resources. By contrast, time is a critical factor in two major aspects. The first is the budget provided to \isklearn, which \irace uses to compute the maximum number of iterations and the number of candidate configurations to be sampled at each iteration. Also, one must set a cutoff time for the evaluation of a given candidate, or else a very expensive model fitting may compromise the configuration.

%Altogether, the In the next section, we assess the proposals discussed above.
%260 --dataset natal-crimes --task regression --sparse False --f_eng1 Selection --f_eng2 None --pre_scaling False --selection SelectFromModel --sel_model SVM --sel_threshold median --scaling False --algorithm LinearRegression
%
%318 --dataset boston-crimes --task regression --sparse False --f_eng1 Selection --f_eng2 Extraction --pre_scaling True --extraction FastICA --ext_components 0.5779 --ica_algorithm deflation --ica_fun exp --selection SelectFromModel --sel_model SVM --sel_threshold median --scaling False --algorithm LinearRegression

\medskip

Altogether, the configuration space and setup proposed in this section render \isklearn a fully-functional AutoML system, which we evaluate in the next section.