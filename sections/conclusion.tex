%!TEX root = ../main.tex

\section{Conclusion}
\label{sec:conclusion}

Automated machine learning~(AutoML) is a growing research field both as to the number of tools available and to the results they help achieve. One of its main goals is to bridge non-experts and the specialized knowledge underlying successful ML applications. Another, equally important, is to reduce the computational/environmental cost currently incurred by the ML industry. Overall, AutoML approaches stem from research fields as diverse as algorithm configuration, neuroevolution, and neural architecture search. Yet, the proposal and assessment of these approaches need to be aware of their cost.

In this work, we have conducted an experimental investigation with these two goals in mind, attempting to maximize the number of insights observed from our work while keeping the number of experiments constrained. In this context, we have empirically demonstrated how \irace can be used to configure pipelines that outperform the predictors that comprise it, considering several relevant application domains, such as computer vision, natural language processing, and time series analysis. Even if the goal of the paper is not to benchmark AutoML approaches, it is remarkable that the pipelines engineered from \isklearn displayed competitive performance w.r.t. more elaborate ensembles produced by the well-known \autosklearn. 
Furthermore, our minimalist configuration space proved helpful both as to effectiveness and to interpretability of pipeline composition. Finally, we assessed different configuration setups and discussed the different options a practitioner can employ to further improve efficacy. 
%Interestingly, the insights we observe confirm what had been reported in the literature about transfer learning between neural networks, even if we consider a larger sample of predictors.

Our work aimed to be comprehensive, and so many of the insights we observed deserve future investigation. The first is pushing further the idea of environment-aware AutoML research. Indirectly, this effort has been driven by the desire to bring machine learning models to mobile hardware. One emblematic example is multi-objective neuroevolution, which evolves models to simultaneously optimize these two goals. Concerning algorithm configuration AutoML approaches, \irace is an important asset in this task, as it 
has been effectively employed for multi-objective configuration~\cite{BezerraPhD,BezLopStu2020}. 

A second path for future work is to investigate machine learning-specific setups for irace. Here, we have observed that variations to the setup adopted affect the performance of the pipelines produced, specially for computer vision problems. A likely promising direction is to mimic deep learning training, where models are repeatedly exposed to the same dataset. In the context of \isklearn, this would translate into resuming training when a candidate pipeline needs to be evaluated on the same meta-fold again. The main challenge with this alternative is to balance effectiveness with the costs it incurs.

% A third path of investigation concerns transfer learning and its impact may not be limited to \isklearn. Specifically, effective transfer learning between neural networks has generally relied on (i)~large-scale source datasets and (ii)~fine-tuning for co-adapting neurons. In this work, neither of these assets have yet been adopted. Concerning the former, it would be trivial to produce variants of existing large image datasets such as ImageNet or Faces in the Wild for small image classification. Regarding fine-tuning, it remains to be investigated how pipelines produced from \isklearn could be used as a source for backpropagation to the reused network.

Finally, while our investigation has focused on AutoML effectiveness, it is also imperative to pursue robustness. More precisely, in this investigation we have sought to include problem datasets from diverse applications domains. Yet, every dataset we have considered was initially subject to some extent of manual data preparation. Other AutoML tools, such as \autosklearn, aim to automate even this part of the process. Considering the importance of data preparation, its relevance to predicting performance, and the reduced computational cost of this stage, making available a data preparation module to be coupled with \isklearn is an important path of future work.

%Automated algorithm engineering approaches seek to bridge the gap between non-experts and the specialized knowledge underlying successful soft computing  applications. The case of automated machine learning~(AutoML) is a rather emblematic successful example, with industry and academia working together to popularize the field. Although many tools are already available for the general public, the efficacy of those tools must be continuously improved by the research community. For this purpose, the algorithm configuration community has devised a number of efficient tools over the past years that have been seldom (if at all) explored in the context of (automated) machine learning. 
%
%\irace is one such tool, having become one of the most used and efficient algorithm configurators currently available. Indeed, it is rather unfortunate that no AutoML tool powered by \irace had been proposed so far, particularly due to its setup flexibility and ability to deal with different types of parameters. In this work, we have conducted a first effort in this direction, covering all aspects required to use \irace in the context of (automated) machine learning.
%
%Although our approach is fairly simple, we have empirically demonstrated how \irace can be used to configure pipelines that outperform the ML predictors that comprise it, considering several relevant application domains, namely computer vision, natural language processing, and time series analysis. The pipelines engineered in this work even displayed competitive performance w.r.t. more elaborate ensembles produced by the well-known \autosklearn. 
%
%Yet, the ultimate goal of AutoML is to help humans devise state-of-the-art algorithms in an automated way, a goal that has been proven feasible in other fields as long as templates and frameworks are enriched with domain-specific components. In future work, we intend to pursue this goal. Fortunately, the machine learning community offers a number of software tools that can be integrated with \isklearn to increase the efficacy of the pipelines one may instantiate from it. In particular, the inclusion of deep learning algorithms is one of the most pressing next steps of this work. Although challenging due to the computational overhead incurred by deep learning, this research direction has the potential to help produce the next breakthroughs in computational intelligence. 
%
%Finally, it is also fairly important to expand the range of application domains considered. To this end, our goal is to provide \isklearn as an open source project which the machine learning community may use and with which it may collaborate. 

